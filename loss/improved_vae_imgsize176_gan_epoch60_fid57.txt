ImprovedVAE_Config()
	batch_size: 32
	beta1: 0.9
	beta2: 0.95
	channels: [64, 64, 128, 256, 256]
	crop_size: 176
	disc_start: 10000
	disc_weight: 0.5
	gpu_num: 3
	grad_clip: 1.0
	gradient_accumulation_steps: 1
	img_size: 176
	kl_weight: 0.0005
	layers: [3, 3, 3, 3]
	learning_rate: 0.0006
	lr_decay_iters: 112000
	max_iters: 112000
	min_lr: 1e-05
	num_epoch: 60
	warmup_iters: 1000
	z_channel: 8

total_params: 23.49M, encoder_params: 10.61M, decoder_params: 10.12M, discriminator_params: 2.77M
/home/work/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/work/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/work/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
step 500, loss 0.335, rec 0.262, kl 0.072, g 0.000, w 1.000, d 0.000, lr: 0.0002994, consume 171.51s
step 1000, loss 0.232, rec 0.188, kl 0.044, g 0.000, w 1.000, d 0.000, lr: 0.0005994, consume 94.69s
step 1500, loss 0.220, rec 0.176, kl 0.044, g 0.000, w 1.000, d 0.000, lr: 0.0006000, consume 94.51s
    valid loss: 0.212, rec 0.164, kl 0.043, g 0.005, w 1.000, d 1.000, consume: 54.348s
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.89it/s]
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.81it/s]
FID:  137.51612634238703
epoch: 0, consume: 525.567s
step 2000, loss 0.207, rec 0.165, kl 0.042, g 0.000, w 1.000, d 0.000, lr: 0.0005999, consume 26.08s
step 2500, loss 0.206, rec 0.164, kl 0.042, g 0.000, w 1.000, d 0.000, lr: 0.0005997, consume 94.91s
step 3000, loss 0.203, rec 0.161, kl 0.042, g 0.000, w 1.000, d 0.000, lr: 0.0005995, consume 94.53s
step 3500, loss 0.199, rec 0.159, kl 0.041, g 0.000, w 1.000, d 0.000, lr: 0.0005993, consume 94.27s
    valid loss: 0.201, rec 0.156, kl 0.040, g 0.005, w 1.000, d 1.000, consume: 31.870s
epoch: 1, consume: 400.455s
step 4000, loss 0.196, rec 0.156, kl 0.040, g 0.000, w 1.000, d 0.000, lr: 0.0005989, consume 44.20s
step 4500, loss 0.196, rec 0.156, kl 0.040, g 0.000, w 1.000, d 0.000, lr: 0.0005986, consume 95.32s
step 5000, loss 0.193, rec 0.154, kl 0.040, g 0.000, w 1.000, d 0.000, lr: 0.0005981, consume 94.69s
step 5500, loss 0.193, rec 0.153, kl 0.040, g 0.000, w 1.000, d 0.000, lr: 0.0005976, consume 94.76s
    valid loss: 0.194, rec 0.150, kl 0.039, g 0.005, w 1.000, d 1.000, consume: 31.840s
epoch: 2, consume: 401.211s
step 6000, loss 0.191, rec 0.152, kl 0.039, g 0.000, w 1.000, d 0.000, lr: 0.0005971, consume 62.42s
step 6500, loss 0.190, rec 0.151, kl 0.039, g 0.000, w 1.000, d 0.000, lr: 0.0005964, consume 94.97s
step 7000, loss 0.189, rec 0.151, kl 0.039, g 0.000, w 1.000, d 0.000, lr: 0.0005958, consume 94.68s
step 7500, loss 0.189, rec 0.151, kl 0.039, g 0.000, w 1.000, d 0.000, lr: 0.0005950, consume 94.73s
    valid loss: 0.193, rec 0.149, kl 0.039, g 0.005, w 1.000, d 1.000, consume: 31.960s
epoch: 3, consume: 400.728s
step 8000, loss 0.188, rec 0.149, kl 0.039, g 0.000, w 1.000, d 0.000, lr: 0.0005942, consume 81.06s
step 8500, loss 0.187, rec 0.149, kl 0.039, g 0.000, w 1.000, d 0.000, lr: 0.0005934, consume 95.05s
step 9000, loss 0.187, rec 0.149, kl 0.039, g 0.000, w 1.000, d 0.000, lr: 0.0005925, consume 95.18s
step 9500, loss 0.186, rec 0.147, kl 0.038, g 0.000, w 1.000, d 0.000, lr: 0.0005915, consume 94.77s
    valid loss: 0.189, rec 0.145, kl 0.039, g 0.005, w 1.000, d 1.000, consume: 32.132s
epoch: 4, consume: 401.908s
step 10000, loss 0.186, rec 0.148, kl 0.038, g 0.000, w 1.000, d 0.000, lr: 0.0005905, consume 99.47s
[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
step 10500, loss 0.196, rec 0.152, kl 0.038, g 0.006, w 16.925, d 0.696, lr: 0.0005894, consume 204.32s
step 11000, loss 0.199, rec 0.157, kl 0.038, g 0.005, w 0.005, d 0.516, lr: 0.0005883, consume 93.78s
    valid loss: 0.781, rec 0.158, kl 0.038, g 0.586, w 1.000, d 0.873, consume: 32.175s
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.82it/s]
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.45it/s]
FID:  156.27084358960113
epoch: 5, consume: 540.982s
step 11500, loss 0.200, rec 0.158, kl 0.038, g 0.005, w 0.004, d 0.543, lr: 0.0005871, consume 22.61s
step 12000, loss 0.203, rec 0.160, kl 0.038, g 0.005, w 0.006, d 0.608, lr: 0.0005858, consume 93.15s
step 12500, loss 0.202, rec 0.159, kl 0.038, g 0.005, w 0.035, d 0.562, lr: 0.0005845, consume 93.40s
step 13000, loss 0.204, rec 0.161, kl 0.038, g 0.005, w 0.007, d 0.626, lr: 0.0005832, consume 93.39s
    valid loss: 0.238, rec 0.173, kl 0.038, g 0.028, w 1.000, d 1.631, consume: 32.014s
epoch: 6, consume: 395.472s
step 13500, loss 0.208, rec 0.164, kl 0.038, g 0.006, w 0.012, d 0.844, lr: 0.0005817, consume 40.88s
step 14000, loss 0.204, rec 0.160, kl 0.038, g 0.005, w 0.008, d 0.602, lr: 0.0005803, consume 93.09s
step 14500, loss 0.210, rec 0.165, kl 0.038, g 0.007, w 0.017, d 0.705, lr: 0.0005787, consume 93.32s
step 15000, loss 0.207, rec 0.163, kl 0.038, g 0.006, w 0.010, d 0.701, lr: 0.0005771, consume 93.31s
    valid loss: 0.454, rec 0.161, kl 0.038, g 0.256, w 1.000, d 0.996, consume: 32.236s
epoch: 7, consume: 395.422s
step 15500, loss 0.208, rec 0.164, kl 0.038, g 0.005, w 0.014, d 0.710, lr: 0.0005755, consume 59.06s
step 16000, loss 0.206, rec 0.163, kl 0.038, g 0.005, w 0.009, d 0.597, lr: 0.0005738, consume 93.10s
step 16500, loss 0.207, rec 0.163, kl 0.038, g 0.005, w 0.009, d 0.701, lr: 0.0005721, consume 93.35s
step 17000, loss 0.209, rec 0.165, kl 0.038, g 0.005, w 0.012, d 0.755, lr: 0.0005703, consume 93.75s
    valid loss: 0.854, rec 0.163, kl 0.038, g 0.653, w 1.000, d 0.881, consume: 32.142s
epoch: 8, consume: 396.430s
step 17500, loss 0.206, rec 0.162, kl 0.038, g 0.006, w 0.027, d 0.639, lr: 0.0005684, consume 77.40s
step 18000, loss 0.207, rec 0.164, kl 0.038, g 0.005, w 0.008, d 0.632, lr: 0.0005665, consume 93.27s
step 18500, loss 0.206, rec 0.162, kl 0.038, g 0.005, w 0.008, d 0.591, lr: 0.0005646, consume 93.19s
step 19000, loss 0.205, rec 0.162, kl 0.038, g 0.005, w 0.008, d 0.692, lr: 0.0005625, consume 93.36s
    valid loss: 0.589, rec 0.155, kl 0.038, g 0.395, w 1.000, d 0.859, consume: 32.243s
epoch: 9, consume: 395.918s
step 19500, loss 0.206, rec 0.162, kl 0.039, g 0.005, w 0.009, d 0.624, lr: 0.0005605, consume 95.04s
step 20000, loss 0.208, rec 0.164, kl 0.038, g 0.006, w 0.011, d 0.698, lr: 0.0005584, consume 93.34s
step 20500, loss 0.207, rec 0.164, kl 0.039, g 0.005, w 0.011, d 0.698, lr: 0.0005562, consume 93.23s
    valid loss: 1.012, rec 0.166, kl 0.038, g 0.808, w 1.000, d 0.919, consume: 32.075s
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.68it/s]
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.61it/s]
FID:  145.39524397755562
epoch: 10, consume: 428.479s
step 21000, loss 0.207, rec 0.164, kl 0.038, g 0.004, w 0.012, d 0.852, lr: 0.0005540, consume 19.89s
step 21500, loss 0.205, rec 0.162, kl 0.039, g 0.005, w 0.009, d 0.603, lr: 0.0005517, consume 93.40s
step 22000, loss 0.210, rec 0.166, kl 0.038, g 0.006, w 0.012, d 0.804, lr: 0.0005494, consume 93.29s
step 22500, loss 0.206, rec 0.162, kl 0.039, g 0.005, w 0.009, d 0.664, lr: 0.0005471, consume 93.51s
    valid loss: 0.460, rec 0.160, kl 0.038, g 0.262, w 1.000, d 0.854, consume: 32.047s
epoch: 11, consume: 395.684s
step 23000, loss 0.208, rec 0.164, kl 0.039, g 0.006, w 0.015, d 0.781, lr: 0.0005446, consume 38.04s
step 23500, loss 0.209, rec 0.164, kl 0.039, g 0.005, w 0.012, d 0.719, lr: 0.0005422, consume 94.24s
step 24000, loss 0.210, rec 0.166, kl 0.039, g 0.005, w 0.014, d 0.823, lr: 0.0005397, consume 93.95s
step 24500, loss 0.207, rec 0.163, kl 0.039, g 0.005, w 0.010, d 0.700, lr: 0.0005371, consume 94.02s
    valid loss: 0.510, rec 0.167, kl 0.039, g 0.304, w 1.000, d 0.800, consume: 31.903s
epoch: 12, consume: 397.863s
step 25000, loss 0.205, rec 0.161, kl 0.039, g 0.006, w 0.009, d 0.633, lr: 0.0005345, consume 56.26s
step 25500, loss 0.205, rec 0.162, kl 0.039, g 0.005, w 0.009, d 0.643, lr: 0.0005319, consume 93.41s
step 26000, loss 0.210, rec 0.166, kl 0.039, g 0.005, w 0.013, d 0.827, lr: 0.0005292, consume 93.57s
step 26500, loss 0.209, rec 0.165, kl 0.039, g 0.006, w 0.016, d 0.825, lr: 0.0005265, consume 93.68s
    valid loss: 0.391, rec 0.157, kl 0.039, g 0.195, w 1.000, d 1.129, consume: 31.930s
epoch: 13, consume: 396.286s
step 27000, loss 0.211, rec 0.166, kl 0.039, g 0.006, w 0.018, d 0.905, lr: 0.0005237, consume 74.53s
step 27500, loss 0.211, rec 0.166, kl 0.039, g 0.006, w 0.018, d 0.862, lr: 0.0005208, consume 93.48s
step 28000, loss 0.210, rec 0.165, kl 0.039, g 0.006, w 0.019, d 0.872, lr: 0.0005180, consume 93.73s
step 28500, loss 0.213, rec 0.167, kl 0.039, g 0.007, w 0.029, d 0.932, lr: 0.0005151, consume 93.59s
    valid loss: 0.519, rec 0.164, kl 0.038, g 0.316, w 1.000, d 0.914, consume: 32.030s
epoch: 14, consume: 396.687s
step 29000, loss 0.210, rec 0.165, kl 0.039, g 0.006, w 0.020, d 0.812, lr: 0.0005121, consume 92.43s
step 29500, loss 0.209, rec 0.164, kl 0.039, g 0.006, w 0.026, d 0.848, lr: 0.0005091, consume 94.15s
step 30000, loss 0.211, rec 0.164, kl 0.039, g 0.007, w 0.024, d 0.876, lr: 0.0005061, consume 93.43s
    valid loss: 0.629, rec 0.167, kl 0.039, g 0.423, w 1.000, d 0.828, consume: 32.194s
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.80it/s]
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.51it/s]
FID:  116.79059986341122
epoch: 15, consume: 430.570s
step 30500, loss 0.206, rec 0.161, kl 0.040, g 0.006, w 0.015, d 0.732, lr: 0.0005030, consume 17.17s
step 31000, loss 0.210, rec 0.164, kl 0.039, g 0.006, w 0.021, d 0.840, lr: 0.0004999, consume 93.44s
step 31500, loss 0.211, rec 0.164, kl 0.039, g 0.007, w 0.024, d 0.887, lr: 0.0004968, consume 93.45s
step 32000, loss 0.212, rec 0.165, kl 0.039, g 0.007, w 0.030, d 0.927, lr: 0.0004936, consume 93.55s
    valid loss: 0.931, rec 0.159, kl 0.039, g 0.732, w 1.000, d 0.914, consume: 32.211s
epoch: 16, consume: 396.365s
step 32500, loss 0.206, rec 0.161, kl 0.040, g 0.005, w 0.015, d 0.810, lr: 0.0004903, consume 35.24s
step 33000, loss 0.210, rec 0.164, kl 0.039, g 0.006, w 0.021, d 0.883, lr: 0.0004871, consume 93.63s
step 33500, loss 0.207, rec 0.162, kl 0.040, g 0.006, w 0.016, d 0.811, lr: 0.0004838, consume 93.68s
step 34000, loss 0.211, rec 0.165, kl 0.039, g 0.007, w 0.027, d 0.930, lr: 0.0004804, consume 93.58s
    valid loss: 0.442, rec 0.157, kl 0.040, g 0.244, w 1.000, d 1.066, consume: 32.015s
epoch: 17, consume: 396.470s
step 34500, loss 0.206, rec 0.161, kl 0.040, g 0.006, w 0.015, d 0.791, lr: 0.0004770, consume 53.62s
step 35000, loss 0.209, rec 0.164, kl 0.039, g 0.006, w 0.021, d 0.881, lr: 0.0004736, consume 93.53s
step 35500, loss 0.210, rec 0.164, kl 0.039, g 0.007, w 0.024, d 0.878, lr: 0.0004702, consume 93.86s
step 36000, loss 0.207, rec 0.162, kl 0.040, g 0.006, w 0.017, d 0.793, lr: 0.0004667, consume 93.77s
    valid loss: 0.618, rec 0.155, kl 0.039, g 0.423, w 1.000, d 0.932, consume: 31.928s
epoch: 18, consume: 397.029s
step 36500, loss 0.207, rec 0.162, kl 0.039, g 0.006, w 0.025, d 0.867, lr: 0.0004632, consume 71.65s
step 37000, loss 0.207, rec 0.161, kl 0.040, g 0.006, w 0.021, d 0.849, lr: 0.0004597, consume 94.00s
step 37500, loss 0.211, rec 0.165, kl 0.040, g 0.007, w 0.027, d 0.931, lr: 0.0004561, consume 93.49s
step 38000, loss 0.210, rec 0.163, kl 0.039, g 0.007, w 0.023, d 0.900, lr: 0.0004525, consume 93.60s
    valid loss: 0.367, rec 0.164, kl 0.039, g 0.163, w 1.000, d 0.910, consume: 31.961s
epoch: 19, consume: 396.786s
step 38500, loss 0.207, rec 0.161, kl 0.040, g 0.006, w 0.018, d 0.815, lr: 0.0004489, consume 89.67s
step 39000, loss 0.211, rec 0.164, kl 0.039, g 0.007, w 0.032, d 0.911, lr: 0.0004452, consume 93.99s
step 39500, loss 0.207, rec 0.160, kl 0.040, g 0.007, w 0.021, d 0.809, lr: 0.0004415, consume 93.57s
    valid loss: 0.505, rec 0.162, kl 0.039, g 0.304, w 1.000, d 0.904, consume: 32.180s
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.75it/s]
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.56it/s]
FID:  128.29894257279483
epoch: 20, consume: 429.692s
step 40000, loss 0.212, rec 0.166, kl 0.039, g 0.008, w 0.033, d 1.056, lr: 0.0004378, consume 14.25s
step 40500, loss 0.207, rec 0.161, kl 0.040, g 0.006, w 0.020, d 0.846, lr: 0.0004341, consume 93.31s
step 41000, loss 0.209, rec 0.163, kl 0.040, g 0.006, w 0.021, d 0.867, lr: 0.0004303, consume 93.63s
step 41500, loss 0.208, rec 0.162, kl 0.040, g 0.006, w 0.021, d 0.874, lr: 0.0004265, consume 94.06s
    valid loss: 0.668, rec 0.165, kl 0.039, g 0.464, w 1.000, d 0.868, consume: 32.200s
epoch: 21, consume: 397.002s
step 42000, loss 0.208, rec 0.162, kl 0.040, g 0.006, w 0.022, d 0.874, lr: 0.0004227, consume 32.65s
step 42500, loss 0.208, rec 0.162, kl 0.040, g 0.007, w 0.023, d 0.897, lr: 0.0004189, consume 93.55s
step 43000, loss 0.206, rec 0.160, kl 0.040, g 0.006, w 0.019, d 0.817, lr: 0.0004150, consume 93.93s
step 43500, loss 0.207, rec 0.161, kl 0.040, g 0.006, w 0.021, d 0.826, lr: 0.0004111, consume 93.47s
    valid loss: 0.414, rec 0.165, kl 0.039, g 0.209, w 1.000, d 0.847, consume: 32.147s
epoch: 22, consume: 396.972s
step 44000, loss 0.209, rec 0.162, kl 0.040, g 0.006, w 0.019, d 0.864, lr: 0.0004072, consume 50.75s
step 44500, loss 0.205, rec 0.159, kl 0.040, g 0.006, w 0.016, d 0.761, lr: 0.0004033, consume 93.71s
step 45000, loss 0.206, rec 0.160, kl 0.040, g 0.006, w 0.019, d 0.838, lr: 0.0003993, consume 93.83s
step 45500, loss 0.203, rec 0.158, kl 0.040, g 0.006, w 0.015, d 0.733, lr: 0.0003954, consume 93.69s
    valid loss: 0.544, rec 0.165, kl 0.039, g 0.340, w 1.000, d 0.882, consume: 32.289s
epoch: 23, consume: 397.296s
step 46000, loss 0.212, rec 0.165, kl 0.040, g 0.007, w 0.030, d 1.008, lr: 0.0003914, consume 68.78s
step 46500, loss 0.207, rec 0.161, kl 0.040, g 0.007, w 0.022, d 0.850, lr: 0.0003874, consume 93.69s
step 47000, loss 0.204, rec 0.159, kl 0.040, g 0.005, w 0.014, d 0.782, lr: 0.0003834, consume 93.39s
step 47500, loss 0.205, rec 0.160, kl 0.040, g 0.005, w 0.017, d 0.800, lr: 0.0003793, consume 93.99s
    valid loss: 0.513, rec 0.161, kl 0.039, g 0.313, w 1.000, d 0.894, consume: 32.128s
epoch: 24, consume: 396.970s
step 48000, loss 0.210, rec 0.163, kl 0.040, g 0.007, w 0.035, d 0.973, lr: 0.0003753, consume 87.26s
step 48500, loss 0.204, rec 0.158, kl 0.040, g 0.006, w 0.014, d 0.750, lr: 0.0003712, consume 93.34s
step 49000, loss 0.205, rec 0.159, kl 0.040, g 0.005, w 0.017, d 0.831, lr: 0.0003672, consume 93.56s
    valid loss: 0.684, rec 0.159, kl 0.042, g 0.483, w 1.000, d 0.966, consume: 32.358s
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.67it/s]
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.49it/s]
FID:  88.3704386881987
epoch: 25, consume: 429.760s
step 49500, loss 0.203, rec 0.157, kl 0.040, g 0.006, w 0.015, d 0.773, lr: 0.0003631, consume 11.93s
step 50000, loss 0.204, rec 0.159, kl 0.040, g 0.006, w 0.015, d 0.801, lr: 0.0003590, consume 93.56s
step 50500, loss 0.205, rec 0.159, kl 0.040, g 0.005, w 0.017, d 0.829, lr: 0.0003549, consume 93.57s
step 51000, loss 0.203, rec 0.157, kl 0.040, g 0.005, w 0.015, d 0.777, lr: 0.0003507, consume 93.65s
    valid loss: 0.757, rec 0.161, kl 0.040, g 0.556, w 1.000, d 0.888, consume: 32.180s
epoch: 26, consume: 397.293s
step 51500, loss 0.206, rec 0.162, kl 0.040, g 0.005, w 0.026, d 0.944, lr: 0.0003466, consume 29.75s
step 52000, loss 0.206, rec 0.160, kl 0.040, g 0.006, w 0.017, d 0.869, lr: 0.0003425, consume 93.70s
step 52500, loss 0.203, rec 0.157, kl 0.040, g 0.006, w 0.016, d 0.727, lr: 0.0003383, consume 93.57s
step 53000, loss 0.203, rec 0.158, kl 0.040, g 0.005, w 0.015, d 0.750, lr: 0.0003342, consume 93.63s
    valid loss: 0.562, rec 0.162, kl 0.039, g 0.361, w 1.000, d 0.843, consume: 32.248s
epoch: 27, consume: 396.913s
step 53500, loss 0.205, rec 0.160, kl 0.040, g 0.005, w 0.021, d 0.896, lr: 0.0003300, consume 48.11s
step 54000, loss 0.205, rec 0.160, kl 0.040, g 0.006, w 0.023, d 0.876, lr: 0.0003259, consume 93.73s
step 54500, loss 0.205, rec 0.160, kl 0.040, g 0.006, w 0.016, d 0.863, lr: 0.0003217, consume 93.70s
step 55000, loss 0.205, rec 0.159, kl 0.040, g 0.006, w 0.018, d 0.820, lr: 0.0003175, consume 94.90s
    valid loss: 0.582, rec 0.162, kl 0.040, g 0.381, w 1.000, d 0.822, consume: 32.297s
epoch: 28, consume: 398.741s
step 55500, loss 0.201, rec 0.156, kl 0.040, g 0.005, w 0.011, d 0.681, lr: 0.0003134, consume 66.02s
step 56000, loss 0.202, rec 0.157, kl 0.040, g 0.005, w 0.011, d 0.782, lr: 0.0003092, consume 93.50s
step 56500, loss 0.203, rec 0.158, kl 0.040, g 0.005, w 0.013, d 0.818, lr: 0.0003050, consume 93.97s
step 57000, loss 0.202, rec 0.157, kl 0.040, g 0.005, w 0.014, d 0.756, lr: 0.0003008, consume 93.58s
    valid loss: 0.608, rec 0.156, kl 0.040, g 0.412, w 1.000, d 0.794, consume: 32.240s
epoch: 29, consume: 396.896s
step 57500, loss 0.204, rec 0.159, kl 0.040, g 0.006, w 0.017, d 0.834, lr: 0.0002967, consume 84.24s
step 58000, loss 0.203, rec 0.158, kl 0.040, g 0.005, w 0.014, d 0.790, lr: 0.0002925, consume 93.56s
step 58500, loss 0.203, rec 0.158, kl 0.040, g 0.005, w 0.015, d 0.784, lr: 0.0002883, consume 93.67s
    valid loss: 0.369, rec 0.151, kl 0.040, g 0.178, w 1.000, d 0.915, consume: 32.056s
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.75it/s]
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.40it/s]
FID:  73.33081960872278
epoch: 30, consume: 430.034s
step 59000, loss 0.199, rec 0.152, kl 0.042, g 0.005, w 0.017, d 0.456, lr: 0.0002842, consume 8.65s
step 59500, loss 0.200, rec 0.154, kl 0.040, g 0.005, w 0.009, d 0.610, lr: 0.0002800, consume 93.80s
step 60000, loss 0.200, rec 0.155, kl 0.040, g 0.005, w 0.010, d 0.684, lr: 0.0002758, consume 93.81s
step 60500, loss 0.200, rec 0.155, kl 0.040, g 0.004, w 0.009, d 0.678, lr: 0.0002717, consume 93.53s
    valid loss: 0.514, rec 0.153, kl 0.040, g 0.320, w 1.000, d 0.810, consume: 32.218s
epoch: 31, consume: 397.073s
step 61000, loss 0.199, rec 0.155, kl 0.040, g 0.004, w 0.008, d 0.676, lr: 0.0002675, consume 26.91s
step 61500, loss 0.200, rec 0.155, kl 0.040, g 0.004, w 0.009, d 0.702, lr: 0.0002634, consume 93.58s
step 62000, loss 0.199, rec 0.155, kl 0.040, g 0.004, w 0.008, d 0.646, lr: 0.0002593, consume 93.50s
step 62500, loss 0.199, rec 0.155, kl 0.040, g 0.004, w 0.009, d 0.700, lr: 0.0002552, consume 93.69s
    valid loss: 0.805, rec 0.155, kl 0.040, g 0.611, w 1.000, d 0.795, consume: 32.039s
epoch: 32, consume: 397.041s
step 63000, loss 0.198, rec 0.153, kl 0.040, g 0.004, w 0.007, d 0.651, lr: 0.0002510, consume 45.01s
step 63500, loss 0.198, rec 0.153, kl 0.040, g 0.004, w 0.007, d 0.553, lr: 0.0002469, consume 93.53s
step 64000, loss 0.199, rec 0.154, kl 0.040, g 0.004, w 0.009, d 0.643, lr: 0.0002429, consume 93.58s
step 64500, loss 0.198, rec 0.154, kl 0.040, g 0.004, w 0.006, d 0.608, lr: 0.0002388, consume 93.71s
    valid loss: 0.790, rec 0.159, kl 0.040, g 0.592, w 1.000, d 0.814, consume: 32.170s
epoch: 33, consume: 396.585s
step 65000, loss 0.199, rec 0.155, kl 0.040, g 0.004, w 0.006, d 0.640, lr: 0.0002347, consume 63.32s
step 65500, loss 0.198, rec 0.154, kl 0.040, g 0.004, w 0.006, d 0.602, lr: 0.0002307, consume 93.49s
step 66000, loss 0.198, rec 0.154, kl 0.040, g 0.004, w 0.006, d 0.586, lr: 0.0002266, consume 94.13s
step 66500, loss 0.197, rec 0.153, kl 0.040, g 0.004, w 0.005, d 0.503, lr: 0.0002226, consume 93.56s
    valid loss: 0.739, rec 0.155, kl 0.039, g 0.544, w 1.000, d 0.770, consume: 32.299s
epoch: 34, consume: 397.341s
step 67000, loss 0.199, rec 0.155, kl 0.040, g 0.004, w 0.007, d 0.681, lr: 0.0002186, consume 81.18s
step 67500, loss 0.198, rec 0.154, kl 0.040, g 0.004, w 0.006, d 0.653, lr: 0.0002146, consume 93.48s
step 68000, loss 0.198, rec 0.154, kl 0.040, g 0.004, w 0.008, d 0.610, lr: 0.0002107, consume 93.51s
step 68500, loss 0.197, rec 0.153, kl 0.040, g 0.004, w 0.005, d 0.512, lr: 0.0002067, consume 93.66s
    valid loss: 0.756, rec 0.154, kl 0.040, g 0.562, w 1.000, d 0.792, consume: 32.051s
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.55it/s]
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.56it/s]
FID:  64.96523824609125
epoch: 35, consume: 429.519s
step 69000, loss 0.197, rec 0.153, kl 0.040, g 0.003, w 0.006, d 0.582, lr: 0.0002028, consume 99.27s
step 69500, loss 0.196, rec 0.152, kl 0.040, g 0.004, w 0.005, d 0.518, lr: 0.0001989, consume 94.12s
step 70000, loss 0.197, rec 0.153, kl 0.040, g 0.004, w 0.005, d 0.524, lr: 0.0001950, consume 93.54s
    valid loss: 1.110, rec 0.154, kl 0.040, g 0.915, w 1.000, d 0.820, consume: 32.098s
epoch: 36, consume: 396.911s
step 70500, loss 0.196, rec 0.153, kl 0.041, g 0.003, w 0.010, d 0.596, lr: 0.0001912, consume 24.03s
step 71000, loss 0.195, rec 0.151, kl 0.040, g 0.004, w 0.005, d 0.488, lr: 0.0001873, consume 93.53s
step 71500, loss 0.195, rec 0.151, kl 0.040, g 0.004, w 0.004, d 0.450, lr: 0.0001835, consume 93.65s
step 72000, loss 0.195, rec 0.151, kl 0.040, g 0.004, w 0.004, d 0.447, lr: 0.0001797, consume 94.03s
    valid loss: 0.708, rec 0.153, kl 0.040, g 0.515, w 1.000, d 0.754, consume: 32.068s
epoch: 37, consume: 397.377s
step 72500, loss 0.195, rec 0.151, kl 0.040, g 0.003, w 0.004, d 0.376, lr: 0.0001759, consume 42.32s
step 73000, loss 0.195, rec 0.151, kl 0.040, g 0.003, w 0.004, d 0.410, lr: 0.0001722, consume 93.50s
step 73500, loss 0.195, rec 0.152, kl 0.040, g 0.003, w 0.005, d 0.507, lr: 0.0001685, consume 93.71s
step 74000, loss 0.195, rec 0.151, kl 0.040, g 0.004, w 0.004, d 0.453, lr: 0.0001648, consume 93.71s
    valid loss: 0.812, rec 0.151, kl 0.040, g 0.621, w 1.000, d 0.737, consume: 32.125s
epoch: 38, consume: 396.970s
step 74500, loss 0.194, rec 0.151, kl 0.040, g 0.003, w 0.004, d 0.476, lr: 0.0001611, consume 60.32s
step 75000, loss 0.194, rec 0.150, kl 0.040, g 0.004, w 0.003, d 0.324, lr: 0.0001575, consume 93.77s
step 75500, loss 0.193, rec 0.150, kl 0.040, g 0.003, w 0.003, d 0.338, lr: 0.0001539, consume 93.54s
step 76000, loss 0.193, rec 0.149, kl 0.040, g 0.003, w 0.003, d 0.330, lr: 0.0001503, consume 93.92s
    valid loss: 0.980, rec 0.150, kl 0.040, g 0.790, w 1.000, d 0.738, consume: 32.191s
epoch: 39, consume: 397.074s
step 76500, loss 0.194, rec 0.150, kl 0.040, g 0.003, w 0.004, d 0.440, lr: 0.0001468, consume 78.50s
step 77000, loss 0.193, rec 0.149, kl 0.041, g 0.003, w 0.004, d 0.312, lr: 0.0001433, consume 93.59s
step 77500, loss 0.192, rec 0.148, kl 0.040, g 0.003, w 0.003, d 0.235, lr: 0.0001398, consume 93.62s
step 78000, loss 0.192, rec 0.149, kl 0.040, g 0.003, w 0.002, d 0.223, lr: 0.0001364, consume 93.96s
    valid loss: 0.932, rec 0.147, kl 0.040, g 0.745, w 1.000, d 0.726, consume: 32.258s
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.62it/s]
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.34it/s]
FID:  59.248461823002344
epoch: 40, consume: 429.938s
step 78500, loss 0.192, rec 0.148, kl 0.041, g 0.003, w 0.004, d 0.289, lr: 0.0001330, consume 97.00s
step 79000, loss 0.193, rec 0.149, kl 0.040, g 0.003, w 0.004, d 0.311, lr: 0.0001296, consume 93.67s
step 79500, loss 0.191, rec 0.148, kl 0.040, g 0.003, w 0.003, d 0.253, lr: 0.0001263, consume 93.45s
    valid loss: 0.677, rec 0.147, kl 0.040, g 0.490, w 1.000, d 0.720, consume: 32.231s
epoch: 41, consume: 396.977s
step 80000, loss 0.193, rec 0.149, kl 0.041, g 0.003, w 0.003, d 0.270, lr: 0.0001229, consume 21.42s
step 80500, loss 0.191, rec 0.148, kl 0.040, g 0.003, w 0.003, d 0.287, lr: 0.0001197, consume 93.38s
step 81000, loss 0.192, rec 0.148, kl 0.040, g 0.003, w 0.003, d 0.282, lr: 0.0001165, consume 93.51s
step 81500, loss 0.192, rec 0.149, kl 0.040, g 0.003, w 0.003, d 0.267, lr: 0.0001133, consume 93.45s
    valid loss: 1.645, rec 0.145, kl 0.040, g 1.459, w 1.000, d 0.897, consume: 32.257s
epoch: 42, consume: 396.465s
step 82000, loss 0.190, rec 0.146, kl 0.040, g 0.004, w 0.002, d 0.161, lr: 0.0001101, consume 39.32s
step 82500, loss 0.190, rec 0.146, kl 0.040, g 0.004, w 0.002, d 0.167, lr: 0.0001070, consume 94.15s
step 83000, loss 0.191, rec 0.147, kl 0.040, g 0.003, w 0.002, d 0.219, lr: 0.0001039, consume 93.72s
step 83500, loss 0.190, rec 0.147, kl 0.040, g 0.003, w 0.002, d 0.206, lr: 0.0001009, consume 93.52s
    valid loss: 0.969, rec 0.147, kl 0.040, g 0.782, w 1.000, d 0.755, consume: 32.233s
epoch: 43, consume: 397.282s
step 84000, loss 0.191, rec 0.147, kl 0.040, g 0.003, w 6.901, d 0.224, lr: 0.0000979, consume 57.82s
step 84500, loss 0.191, rec 0.146, kl 0.040, g 0.004, w 0.003, d 0.197, lr: 0.0000949, consume 93.79s
step 85000, loss 0.191, rec 0.147, kl 0.041, g 0.003, w 0.003, d 0.173, lr: 0.0000920, consume 93.45s
step 85500, loss 0.191, rec 0.147, kl 0.040, g 0.003, w 0.002, d 0.137, lr: 0.0000892, consume 93.62s
    valid loss: 1.643, rec 0.148, kl 0.040, g 1.456, w 1.000, d 0.892, consume: 32.146s
epoch: 44, consume: 396.996s
step 86000, loss 0.190, rec 0.146, kl 0.040, g 0.003, w 0.002, d 0.159, lr: 0.0000863, consume 75.72s
step 86500, loss 0.189, rec 0.145, kl 0.040, g 0.004, w 0.002, d 0.134, lr: 0.0000836, consume 93.64s
step 87000, loss 0.190, rec 0.147, kl 0.040, g 0.004, w 0.002, d 0.188, lr: 0.0000808, consume 93.52s
step 87500, loss 0.189, rec 0.145, kl 0.040, g 0.004, w 0.002, d 0.122, lr: 0.0000781, consume 93.51s
    valid loss: 1.353, rec 0.147, kl 0.040, g 1.165, w 1.000, d 0.771, consume: 32.275s
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.55it/s]
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.40it/s]
FID:  56.94028536781542
epoch: 45, consume: 429.600s
step 88000, loss 0.190, rec 0.146, kl 0.040, g 0.003, w 0.002, d 0.121, lr: 0.0000755, consume 93.74s
step 88500, loss 0.189, rec 0.145, kl 0.040, g 0.004, w 0.002, d 0.103, lr: 0.0000729, consume 93.92s
step 89000, loss 0.189, rec 0.144, kl 0.040, g 0.004, w 0.002, d 0.092, lr: 0.0000703, consume 95.02s
    valid loss: 1.109, rec 0.147, kl 0.040, g 0.922, w 1.000, d 0.726, consume: 32.535s
epoch: 46, consume: 399.280s
step 89500, loss 0.190, rec 0.146, kl 0.040, g 0.003, w 0.002, d 0.093, lr: 0.0000678, consume 18.69s
step 90000, loss 0.188, rec 0.144, kl 0.040, g 0.004, w 0.002, d 0.083, lr: 0.0000654, consume 94.42s
step 90500, loss 0.188, rec 0.144, kl 0.040, g 0.004, w 0.002, d 0.086, lr: 0.0000630, consume 94.28s
step 91000, loss 0.189, rec 0.145, kl 0.040, g 0.004, w 0.002, d 0.111, lr: 0.0000606, consume 93.90s
    valid loss: 0.883, rec 0.148, kl 0.040, g 0.696, w 1.000, d 0.699, consume: 32.344s
epoch: 47, consume: 398.972s
step 91500, loss 0.189, rec 0.145, kl 0.041, g 0.004, w 0.001, d 0.082, lr: 0.0000583, consume 36.72s
step 92000, loss 0.188, rec 0.143, kl 0.040, g 0.004, w 0.001, d 0.078, lr: 0.0000560, consume 94.00s
step 92500, loss 0.188, rec 0.144, kl 0.041, g 0.003, w 0.002, d 0.086, lr: 0.0000538, consume 93.93s
step 93000, loss 0.188, rec 0.144, kl 0.040, g 0.004, w 0.002, d 0.068, lr: 0.0000516, consume 93.67s
    valid loss: 0.950, rec 0.145, kl 0.040, g 0.765, w 1.000, d 0.713, consume: 32.259s
epoch: 48, consume: 397.853s
step 93500, loss 0.188, rec 0.144, kl 0.040, g 0.004, w 0.001, d 0.076, lr: 0.0000495, consume 54.85s
step 94000, loss 0.187, rec 0.143, kl 0.040, g 0.004, w 0.002, d 0.059, lr: 0.0000475, consume 93.75s
step 94500, loss 0.187, rec 0.143, kl 0.040, g 0.004, w 0.002, d 0.065, lr: 0.0000455, consume 93.62s
step 95000, loss 0.188, rec 0.144, kl 0.040, g 0.004, w 0.001, d 0.053, lr: 0.0000435, consume 93.66s
    valid loss: 1.051, rec 0.144, kl 0.040, g 0.868, w 1.000, d 0.731, consume: 32.123s
epoch: 49, consume: 397.383s
step 95500, loss 0.188, rec 0.143, kl 0.041, g 0.004, w 0.001, d 0.082, lr: 0.0000416, consume 73.13s
step 96000, loss 0.188, rec 0.143, kl 0.040, g 0.004, w 0.002, d 0.059, lr: 0.0000397, consume 94.05s
step 96500, loss 0.188, rec 0.144, kl 0.040, g 0.004, w 0.001, d 0.068, lr: 0.0000379, consume 94.14s
step 97000, loss 0.187, rec 0.142, kl 0.040, g 0.004, w 0.001, d 0.042, lr: 0.0000362, consume 93.64s
    valid loss: 1.666, rec 0.143, kl 0.040, g 1.483, w 1.000, d 0.895, consume: 32.207s
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.59it/s]
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.39it/s]
FID:  57.99175398530423
epoch: 50, consume: 431.274s
step 97500, loss 0.187, rec 0.142, kl 0.040, g 0.004, w 0.002, d 0.077, lr: 0.0000345, consume 90.95s
step 98000, loss 0.187, rec 0.142, kl 0.041, g 0.004, w 0.001, d 0.044, lr: 0.0000329, consume 93.70s
step 98500, loss 0.187, rec 0.143, kl 0.040, g 0.004, w 0.002, d 0.049, lr: 0.0000313, consume 93.61s
    valid loss: 1.534, rec 0.143, kl 0.040, g 1.350, w 1.000, d 0.817, consume: 32.219s
epoch: 51, consume: 396.747s
step 99000, loss 0.185, rec 0.141, kl 0.040, g 0.004, w 0.001, d 0.027, lr: 0.0000297, consume 15.86s
step 99500, loss 0.186, rec 0.142, kl 0.040, g 0.004, w 0.001, d 0.040, lr: 0.0000283, consume 93.62s
step 100000, loss 0.187, rec 0.143, kl 0.040, g 0.004, w 0.001, d 0.044, lr: 0.0000269, consume 93.78s
step 100500, loss 0.187, rec 0.142, kl 0.040, g 0.004, w 0.001, d 0.038, lr: 0.0000255, consume 93.70s
    valid loss: 2.381, rec 0.143, kl 0.040, g 2.198, w 1.000, d 1.200, consume: 32.005s
epoch: 52, consume: 397.275s
step 101000, loss 0.184, rec 0.140, kl 0.040, g 0.004, w 0.001, d 0.042, lr: 0.0000242, consume 33.69s
step 101500, loss 0.185, rec 0.141, kl 0.040, g 0.004, w 0.001, d 0.038, lr: 0.0000229, consume 94.03s
step 102000, loss 0.186, rec 0.142, kl 0.041, g 0.004, w 0.001, d 0.037, lr: 0.0000217, consume 93.99s
step 102500, loss 0.186, rec 0.142, kl 0.041, g 0.004, w 0.001, d 0.019, lr: 0.0000206, consume 94.10s
    valid loss: 1.924, rec 0.142, kl 0.040, g 1.742, w 1.000, d 1.015, consume: 32.185s
epoch: 53, consume: 398.057s
step 103000, loss 0.185, rec 0.141, kl 0.040, g 0.004, w 0.001, d 0.029, lr: 0.0000195, consume 52.06s
step 103500, loss 0.186, rec 0.142, kl 0.041, g 0.004, w 0.001, d 0.032, lr: 0.0000185, consume 93.80s
step 104000, loss 0.186, rec 0.142, kl 0.041, g 0.004, w 0.001, d 0.026, lr: 0.0000175, consume 94.07s
step 104500, loss 0.186, rec 0.141, kl 0.041, g 0.004, w 0.001, d 0.024, lr: 0.0000166, consume 94.25s
    valid loss: 1.806, rec 0.142, kl 0.040, g 1.624, w 1.000, d 0.956, consume: 32.366s
epoch: 54, consume: 398.517s
step 105000, loss 0.186, rec 0.141, kl 0.041, g 0.004, w 0.001, d 0.017, lr: 0.0000158, consume 70.51s
step 105500, loss 0.186, rec 0.141, kl 0.041, g 0.004, w 0.001, d 0.018, lr: 0.0000150, consume 94.18s
step 106000, loss 0.185, rec 0.141, kl 0.040, g 0.004, w 0.001, d 0.020, lr: 0.0000142, consume 94.08s
step 106500, loss 0.186, rec 0.141, kl 0.040, g 0.004, w 0.001, d 0.015, lr: 0.0000136, consume 93.68s
    valid loss: 1.606, rec 0.142, kl 0.040, g 1.424, w 1.000, d 0.873, consume: 32.405s
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.80it/s]
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.32it/s]
FID:  58.21716041922832
epoch: 55, consume: 432.137s
step 107000, loss 0.185, rec 0.141, kl 0.040, g 0.004, w 0.001, d 0.020, lr: 0.0000130, consume 88.73s
step 107500, loss 0.186, rec 0.141, kl 0.041, g 0.004, w 0.001, d 0.023, lr: 0.0000124, consume 95.34s
step 108000, loss 0.186, rec 0.141, kl 0.041, g 0.004, w 0.001, d 0.023, lr: 0.0000119, consume 94.46s
    valid loss: 2.286, rec 0.142, kl 0.040, g 2.105, w 1.000, d 1.185, consume: 32.530s
epoch: 56, consume: 401.239s
step 108500, loss 0.184, rec 0.140, kl 0.040, g 0.004, w 0.001, d 0.026, lr: 0.0000114, consume 13.09s
step 109000, loss 0.185, rec 0.140, kl 0.040, g 0.004, w 0.001, d 0.022, lr: 0.0000111, consume 93.84s
step 109500, loss 0.185, rec 0.140, kl 0.040, g 0.005, w 0.001, d 0.018, lr: 0.0000107, consume 94.07s
step 110000, loss 0.184, rec 0.139, kl 0.040, g 0.004, w 0.001, d 0.014, lr: 0.0000105, consume 93.96s
    valid loss: 1.347, rec 0.142, kl 0.040, g 1.165, w 1.000, d 0.786, consume: 32.683s
epoch: 57, consume: 398.737s
step 110500, loss 0.186, rec 0.141, kl 0.041, g 0.004, w 0.001, d 0.020, lr: 0.0000103, consume 31.47s
step 111000, loss 0.184, rec 0.140, kl 0.040, g 0.004, w 0.001, d 0.014, lr: 0.0000101, consume 93.77s
step 111500, loss 0.185, rec 0.140, kl 0.041, g 0.004, w 0.001, d 0.014, lr: 0.0000100, consume 93.66s
step 112000, loss 0.185, rec 0.141, kl 0.040, g 0.004, w 0.001, d 0.010, lr: 0.0000100, consume 93.76s
    valid loss: 1.419, rec 0.141, kl 0.040, g 1.238, w 1.000, d 0.809, consume: 31.670s
epoch: 58, consume: 345.232s
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.77it/s]
100%|█████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.63it/s]
FID:  57.29698722962968
